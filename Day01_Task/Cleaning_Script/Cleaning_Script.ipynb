{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c3279ed-6e8e-4f46-8ce4-e5695063fc1a",
   "metadata": {},
   "source": [
    "# ----------------------------\n",
    "#  DATA CLEANING & PREPROCESSING - TASK 1\n",
    "# Dataset: Customer Personality Analysis\n",
    "# Objective: Clean, fix, and prepare dataset for analysis\n",
    "# ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c086263-bc1c-45fa-8a1e-7a26dde31a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8545178-c443-4fbb-85f8-c881a12ecaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"marketing_campaign.csv\") #load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f165bf64-0f68-4397-b9f9-712ceb352381",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFirst 5 Rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nColumn Names:\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "771f4cc3-d31f-4e3a-83b5-0b003d0181ed",
   "metadata": {},
   "source": [
    "While inspecting the first 5 rows and column names, I noticed that each column name had a “t” attached — for example, tYear_Birth, tEducation, and tMarital_Status.\n",
    "To fix it, I reloaded the dataset using the sep='\\t' parameter in pd.read_csv(), which correctly split the columns and removed the unwanted tab characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "829792eb-29bc-43b6-8fc9-35a2ce11baea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"marketing_campaign.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8971464e-1a00-4771-bd76-2fa39c5a9030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 Rows:\n",
      "     ID  Year_Birth   Education Marital_Status   Income  Kidhome  Teenhome  \\\n",
      "0  5524        1957  Graduation         Single  58138.0        0         0   \n",
      "1  2174        1954  Graduation         Single  46344.0        1         1   \n",
      "2  4141        1965  Graduation       Together  71613.0        0         0   \n",
      "3  6182        1984  Graduation       Together  26646.0        1         0   \n",
      "4  5324        1981         PhD        Married  58293.0        1         0   \n",
      "\n",
      "  Dt_Customer  Recency  MntWines  ...  NumWebVisitsMonth  AcceptedCmp3  \\\n",
      "0  04-09-2012       58       635  ...                  7             0   \n",
      "1  08-03-2014       38        11  ...                  5             0   \n",
      "2  21-08-2013       26       426  ...                  4             0   \n",
      "3  10-02-2014       26        11  ...                  6             0   \n",
      "4  19-01-2014       94       173  ...                  5             0   \n",
      "\n",
      "   AcceptedCmp4  AcceptedCmp5  AcceptedCmp1  AcceptedCmp2  Complain  \\\n",
      "0             0             0             0             0         0   \n",
      "1             0             0             0             0         0   \n",
      "2             0             0             0             0         0   \n",
      "3             0             0             0             0         0   \n",
      "4             0             0             0             0         0   \n",
      "\n",
      "   Z_CostContact  Z_Revenue  Response  \n",
      "0              3         11         1  \n",
      "1              3         11         0  \n",
      "2              3         11         0  \n",
      "3              3         11         0  \n",
      "4              3         11         0  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "\n",
      "Column Names:\n",
      "Index(['ID', 'Year_Birth', 'Education', 'Marital_Status', 'Income', 'Kidhome',\n",
      "       'Teenhome', 'Dt_Customer', 'Recency', 'MntWines', 'MntFruits',\n",
      "       'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts',\n",
      "       'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases',\n",
      "       'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth',\n",
      "       'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1',\n",
      "       'AcceptedCmp2', 'Complain', 'Z_CostContact', 'Z_Revenue', 'Response'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFirst 5 Rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nColumn Names:\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c16a16c8-7642-4e6c-85f4-fdb8d0edbf68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column:\n",
      "ID                      0\n",
      "Year_Birth              0\n",
      "Education               0\n",
      "Marital_Status          0\n",
      "Income                 24\n",
      "Kidhome                 0\n",
      "Teenhome                0\n",
      "Dt_Customer             0\n",
      "Recency                 0\n",
      "MntWines                0\n",
      "MntFruits               0\n",
      "MntMeatProducts         0\n",
      "MntFishProducts         0\n",
      "MntSweetProducts        0\n",
      "MntGoldProds            0\n",
      "NumDealsPurchases       0\n",
      "NumWebPurchases         0\n",
      "NumCatalogPurchases     0\n",
      "NumStorePurchases       0\n",
      "NumWebVisitsMonth       0\n",
      "AcceptedCmp3            0\n",
      "AcceptedCmp4            0\n",
      "AcceptedCmp5            0\n",
      "AcceptedCmp1            0\n",
      "AcceptedCmp2            0\n",
      "Complain                0\n",
      "Z_CostContact           0\n",
      "Z_Revenue               0\n",
      "Response                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMissing values per column:\") #Check Missing Value\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afec1eb0-b7ad-4672-8407-37a8d3c2b302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Value %:\n",
      "Income    1.071429\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "null_percent = (df.isnull().sum() / len(df)) * 100\n",
    "print(\"\\nMissing Value %:\")\n",
    "print(null_percent[null_percent > 0])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "924e1d3c-0514-41ed-8d01-1f8a34b8d557",
   "metadata": {},
   "source": [
    "Here, I calculated the percentage of missing values in each column to understand how much data was missing.\n",
    "This helps decide whether to fill the missing values or drop the column entirely, depending on how large the gap is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81be2119-2325-4221-a3e0-360f87489797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filled missing Income values with median: 51381.5\n",
      "Remaining nulls in Income: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SWARAJ\\AppData\\Local\\Temp\\ipykernel_28204\\4148435703.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Income'].fillna(median_income, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "median_income = df['Income'].median()\n",
    "df['Income'].fillna(median_income, inplace=True)\n",
    "print(\"\\nFilled missing Income values with median:\", median_income)\n",
    "print(\"Remaining nulls in Income:\", df['Income'].isnull().sum())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0119e0d1-b4ba-4ef6-9363-b6c63983bfd9",
   "metadata": {},
   "source": [
    "So i Fill the missing value using median "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfb3ffd4-28ed-4900-bec6-eb95b2236f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate Rows Found: 0\n"
     ]
    }
   ],
   "source": [
    "duplicates = df.duplicated().sum()\n",
    "print(\"\\nDuplicate Rows Found:\", duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c089a939-578c-4bc6-800b-c656167fb787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So after Null Values , I check for Duplicate values \n",
    "# There were no duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d80cb4f-0feb-462e-a90b-64fa4044c1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Identify Text Columns\n",
    "# Checking which columns contain text data so I know which ones might need cleaning or formatting later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15c7347a-bab5-4305-bb85-266aeb771b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Columns: ['Education', 'Marital_Status', 'Dt_Customer']\n"
     ]
    }
   ],
   "source": [
    "text_cols = df.select_dtypes(include='object').columns\n",
    "print(\"\\nText Columns:\", text_cols.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2af21b93-89f0-4597-a762-fe3698c17adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking Unique Values in Text Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82e997b8-ad4c-40e9-925e-af27c0fac4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column: Education\n",
      "['Graduation' 'PhD' 'Master' 'Basic' '2n Cycle']\n",
      "\n",
      "Column: Marital_Status\n",
      "['Single' 'Together' 'Married' 'Divorced' 'Widow' 'Alone' 'Absurd' 'YOLO']\n",
      "\n",
      "Column: Dt_Customer\n",
      "['04-09-2012' '08-03-2014' '21-08-2013' '10-02-2014' '19-01-2014'\n",
      " '09-09-2013' '13-11-2012' '08-05-2013' '06-06-2013' '13-03-2014'\n",
      " '15-11-2013' '10-10-2012' '24-11-2012' '24-12-2012' '31-08-2012'\n",
      " '28-03-2013' '03-11-2012' '08-08-2012' '06-01-2013' '23-12-2012'\n",
      " '11-01-2014' '18-03-2013' '02-01-2013' '27-05-2013' '20-02-2013'\n",
      " '31-05-2013' '22-11-2013' '22-05-2014' '11-05-2013' '29-10-2012'\n",
      " '29-08-2013' '31-12-2013' '02-09-2013' '11-02-2014' '01-02-2013'\n",
      " '29-04-2013' '12-03-2013' '05-11-2013' '02-10-2013' '28-06-2014'\n",
      " '09-11-2012' '24-05-2013' '01-01-2014' '08-11-2012' '12-05-2014'\n",
      " '11-08-2012' '07-06-2014' '12-06-2013' '19-11-2012' '02-04-2013'\n",
      " '28-04-2014' '17-06-2013' '03-03-2014' '04-07-2013' '07-09-2012'\n",
      " '18-02-2013' '11-06-2013' '06-12-2013' '21-05-2013' '11-05-2014'\n",
      " '19-03-2014' '27-09-2013' '08-04-2013' '11-09-2012' '14-09-2012'\n",
      " '17-03-2013' '05-04-2013' '30-04-2014' '19-12-2012' '27-08-2012'\n",
      " '12-10-2012' '04-09-2013' '29-08-2012' '23-06-2013' '03-07-2013'\n",
      " '25-02-2014' '11-08-2013' '16-07-2013' '28-05-2014' '21-01-2014'\n",
      " '27-05-2014' '23-11-2013' '23-03-2014' '24-05-2014' '22-11-2012'\n",
      " '11-04-2013' '01-12-2013' '20-06-2013' '23-07-2013' '30-03-2014'\n",
      " '20-04-2013' '17-05-2013' '08-05-2014' '10-12-2013' '24-09-2013'\n",
      " '02-02-2013' '07-12-2012' '02-11-2013' '10-11-2012' '25-06-2014'\n",
      " '12-12-2012' '10-09-2013' '24-01-2014' '19-02-2013' '11-07-2013'\n",
      " '14-11-2013' '24-02-2014' '22-04-2013' '21-04-2013' '08-06-2014'\n",
      " '10-05-2014' '30-09-2013' '10-03-2013' '14-08-2013' '07-07-2013'\n",
      " '19-05-2014' '28-07-2013' '19-10-2012' '19-04-2013' '28-08-2013'\n",
      " '19-03-2013' '18-10-2012' '28-10-2012' '22-08-2012' '21-06-2014'\n",
      " '24-04-2014' '07-03-2014' '14-12-2012' '09-10-2012' '08-07-2013'\n",
      " '12-09-2013' '17-10-2013' '20-08-2013' '01-07-2013' '05-11-2012'\n",
      " '05-01-2014' '01-05-2013' '01-03-2014' '13-11-2013' '18-11-2013'\n",
      " '21-04-2014' '13-07-2013' '30-01-2014' '04-04-2014' '12-09-2012'\n",
      " '16-12-2012' '23-05-2014' '24-06-2014' '28-09-2013' '22-01-2014'\n",
      " '15-06-2014' '05-12-2012' '02-08-2013' '23-02-2013' '09-09-2012'\n",
      " '26-09-2013' '30-05-2013' '29-12-2013' '13-05-2014' '19-09-2013'\n",
      " '17-01-2013' '31-03-2014' '29-06-2014' '09-12-2013' '14-10-2013'\n",
      " '02-11-2012' '17-07-2013' '08-12-2013' '13-05-2013' '10-09-2012'\n",
      " '31-03-2013' '18-03-2014' '05-10-2012' '21-01-2013' '04-05-2013'\n",
      " '01-04-2014' '31-08-2013' '14-11-2012' '11-04-2014' '14-04-2014'\n",
      " '05-01-2013' '08-09-2012' '26-03-2013' '25-10-2012' '09-12-2012'\n",
      " '16-02-2014' '20-03-2013' '15-05-2013' '16-04-2014' '23-03-2013'\n",
      " '04-12-2013' '04-10-2013' '26-12-2013' '17-05-2014' '28-10-2013'\n",
      " '31-07-2013' '28-11-2013' '17-08-2012' '05-06-2014' '20-05-2013'\n",
      " '09-01-2013' '16-09-2013' '27-10-2013' '22-09-2012' '13-10-2012'\n",
      " '16-10-2012' '22-10-2012' '04-06-2013' '22-05-2013' '17-06-2014'\n",
      " '23-11-2012' '03-02-2013' '14-03-2013' '26-06-2014' '15-10-2012'\n",
      " '19-06-2013' '20-03-2014' '04-02-2014' '06-04-2014' '06-02-2013'\n",
      " '11-02-2013' '17-04-2014' '12-07-2013' '29-01-2013' '08-01-2013'\n",
      " '13-06-2013' '27-12-2013' '31-10-2012' '15-01-2014' '23-08-2012'\n",
      " '07-03-2013' '13-01-2013' '12-01-2013' '17-03-2014' '12-10-2013'\n",
      " '13-04-2014' '18-09-2012' '05-03-2014' '27-04-2013' '18-01-2014'\n",
      " '03-06-2013' '17-12-2013' '11-03-2014' '29-07-2013' '14-08-2012'\n",
      " '23-08-2013' '09-02-2014' '07-02-2013' '11-01-2013' '05-07-2013'\n",
      " '02-07-2013' '07-11-2013' '09-05-2013' '13-02-2013' '16-04-2013'\n",
      " '11-09-2013' '03-04-2013' '10-01-2013' '30-06-2013' '06-12-2012'\n",
      " '12-11-2012' '03-03-2013' '10-08-2012' '07-12-2013' '15-08-2013'\n",
      " '10-11-2013' '16-06-2014' '25-12-2012' '03-01-2014' '27-10-2012'\n",
      " '22-12-2012' '29-11-2013' '08-10-2013' '28-09-2012' '22-03-2014'\n",
      " '28-12-2012' '21-08-2012' '16-03-2013' '17-11-2012' '01-12-2012'\n",
      " '22-04-2014' '11-11-2012' '22-06-2013' '18-08-2012' '30-12-2012'\n",
      " '14-06-2013' '16-10-2013' '30-08-2012' '04-05-2014' '18-04-2013'\n",
      " '06-10-2013' '15-09-2012' '27-09-2012' '11-03-2013' '22-10-2013'\n",
      " '09-06-2014' '30-05-2014' '17-10-2012' '30-03-2013' '23-01-2013'\n",
      " '20-11-2013' '14-02-2014' '22-02-2013' '05-03-2013' '06-05-2014'\n",
      " '13-04-2013' '05-04-2014' '25-04-2013' '25-11-2013' '02-02-2014'\n",
      " '21-06-2013' '21-12-2013' '07-09-2013' '22-08-2013' '20-12-2013'\n",
      " '06-08-2013' '09-10-2013' '09-06-2013' '22-12-2013' '02-05-2013'\n",
      " '16-02-2013' '20-08-2012' '01-04-2013' '25-05-2014' '25-09-2012'\n",
      " '22-09-2013' '28-01-2014' '23-04-2013' '06-03-2014' '15-08-2012'\n",
      " '27-03-2013' '10-10-2013' '04-08-2012' '18-05-2014' '09-05-2014'\n",
      " '23-10-2012' '30-12-2013' '19-08-2012' '30-10-2013' '26-06-2013'\n",
      " '26-01-2014' '24-09-2012' '03-12-2012' '27-11-2013' '06-08-2012'\n",
      " '29-12-2012' '24-08-2012' '07-11-2012' '02-12-2013' '19-07-2013'\n",
      " '25-05-2013' '20-10-2013' '14-04-2013' '27-01-2014' '16-08-2013'\n",
      " '30-09-2012' '04-12-2012' '12-08-2012' '03-11-2013' '15-02-2013'\n",
      " '18-09-2013' '18-06-2014' '24-04-2013' '15-02-2014' '08-08-2013'\n",
      " '23-12-2013' '28-11-2012' '07-08-2013' '07-01-2014' '03-12-2013'\n",
      " '29-03-2014' '08-04-2014' '07-02-2014' '06-11-2013' '01-03-2013'\n",
      " '03-08-2013' '16-06-2013' '25-06-2013' '13-09-2013' '01-08-2013'\n",
      " '31-01-2013' '09-07-2013' '15-04-2013' '03-04-2014' '18-04-2014'\n",
      " '23-09-2012' '14-09-2013' '26-10-2012' '19-10-2013' '21-02-2013'\n",
      " '17-09-2013' '26-01-2013' '07-05-2014' '19-08-2013' '18-07-2013'\n",
      " '11-12-2012' '26-07-2013' '06-05-2013' '14-07-2013' '10-03-2014'\n",
      " '28-12-2013' '20-01-2013' '21-09-2012' '06-09-2012' '18-12-2012'\n",
      " '19-11-2013' '13-06-2014' '07-05-2013' '18-08-2013' '13-12-2013'\n",
      " '17-02-2014' '03-08-2012' '04-01-2014' '21-10-2013' '25-12-2013'\n",
      " '01-01-2013' '16-05-2013' '25-09-2013' '08-11-2013' '16-12-2013'\n",
      " '30-10-2012' '02-01-2014' '18-10-2013' '25-07-2013' '24-10-2013'\n",
      " '19-12-2013' '16-05-2014' '04-10-2012' '25-08-2013' '29-05-2013'\n",
      " '26-03-2014' '09-04-2014' '12-03-2014' '29-04-2014' '27-06-2014'\n",
      " '10-04-2013' '21-11-2013' '21-10-2012' '14-05-2013' '01-08-2012'\n",
      " '25-01-2014' '11-10-2013' '07-08-2012' '29-03-2013' '03-06-2014'\n",
      " '09-08-2012' '06-03-2013' '12-05-2013' '09-03-2014' '15-09-2013'\n",
      " '29-01-2014' '03-10-2013' '28-02-2013' '02-05-2014' '09-02-2013'\n",
      " '15-03-2013' '10-05-2013' '05-09-2012' '01-09-2013' '26-05-2013'\n",
      " '12-12-2013' '08-12-2012' '30-08-2013' '18-12-2013' '18-06-2013'\n",
      " '29-05-2014' '15-01-2013' '29-10-2013' '02-03-2013' '14-12-2013'\n",
      " '19-01-2013' '06-04-2013' '04-11-2013' '22-03-2013' '31-05-2014'\n",
      " '04-02-2013' '05-05-2013' '14-02-2013' '21-03-2014' '05-08-2013'\n",
      " '29-11-2012' '27-07-2013' '13-01-2014' '29-09-2012' '23-10-2013'\n",
      " '06-11-2012' '26-05-2014' '04-03-2014' '06-01-2014' '11-06-2014'\n",
      " '19-06-2014' '26-11-2012' '11-12-2013' '14-01-2014' '23-01-2014'\n",
      " '24-03-2013' '29-06-2013' '25-01-2013' '17-11-2013' '18-05-2013'\n",
      " '17-04-2013' '24-01-2013' '09-08-2013' '10-06-2013' '20-07-2013'\n",
      " '28-05-2013' '04-03-2013' '01-10-2012' '10-02-2013' '14-10-2012'\n",
      " '03-02-2014' '22-06-2014' '25-10-2013' '18-02-2014' '09-11-2013'\n",
      " '13-08-2013' '25-08-2012' '19-04-2014' '07-04-2014' '03-01-2013'\n",
      " '04-11-2012' '22-01-2013' '26-02-2014' '23-04-2014' '30-07-2013'\n",
      " '13-10-2013' '27-11-2012' '01-05-2014' '30-11-2012' '17-08-2013'\n",
      " '04-01-2013' '03-05-2014' '26-04-2014' '02-06-2014' '26-10-2013'\n",
      " '24-03-2014' '25-02-2013' '20-09-2013' '16-01-2013' '24-12-2013'\n",
      " '18-11-2012' '05-08-2012' '03-05-2013' '12-11-2013' '06-07-2013'\n",
      " '21-07-2013' '20-10-2012' '23-05-2013' '26-08-2012' '12-01-2014'\n",
      " '13-02-2014' '27-02-2014' '20-04-2014' '28-06-2013' '28-04-2013'\n",
      " '21-03-2013' '27-08-2013' '30-07-2012' '05-12-2013' '08-02-2013'\n",
      " '21-11-2012' '26-02-2013' '20-09-2012' '08-06-2013' '05-09-2013'\n",
      " '12-04-2014' '24-06-2013' '19-02-2014' '27-12-2012' '01-11-2012'\n",
      " '22-07-2013' '31-01-2014' '26-08-2013' '01-10-2013' '10-06-2014'\n",
      " '01-02-2014' '03-09-2012' '26-09-2012' '31-07-2012' '02-12-2012'\n",
      " '05-05-2014' '06-10-2012' '02-03-2014' '15-05-2014' '10-12-2012'\n",
      " '13-08-2012' '23-02-2014' '16-03-2014' '21-09-2013' '24-10-2012'\n",
      " '15-10-2013' '17-02-2013' '28-08-2012' '12-02-2014' '08-02-2014'\n",
      " '01-09-2012' '16-11-2012' '12-02-2013' '02-08-2012' '16-08-2012'\n",
      " '25-11-2012' '27-04-2014' '30-04-2013' '20-11-2012' '14-06-2014'\n",
      " '28-03-2014' '08-03-2013' '02-09-2012' '15-04-2014' '25-04-2014'\n",
      " '27-06-2013' '01-06-2013' '05-10-2013' '22-02-2014' '12-04-2013'\n",
      " '23-09-2013' '24-07-2013' '23-06-2014' '10-01-2014' '25-03-2013'\n",
      " '17-01-2014' '04-06-2014' '25-03-2014' '31-12-2012' '19-09-2012'\n",
      " '27-02-2013' '02-10-2012' '09-04-2013' '02-04-2014' '07-01-2013'\n",
      " '26-11-2013' '04-08-2013' '17-12-2012' '06-06-2014' '19-05-2013'\n",
      " '12-08-2013' '15-12-2013' '07-10-2012' '06-09-2013' '15-07-2013'\n",
      " '27-01-2013' '05-02-2014' '26-12-2012' '06-02-2014' '14-01-2013'\n",
      " '20-06-2014' '24-08-2013' '28-02-2014' '07-04-2013' '10-04-2014'\n",
      " '12-06-2014' '30-11-2013' '09-03-2013' '27-03-2014' '15-12-2012'\n",
      " '17-09-2012' '02-06-2013' '21-12-2012' '01-11-2013' '10-08-2013'\n",
      " '11-10-2012' '20-12-2012' '09-01-2014']\n"
     ]
    }
   ],
   "source": [
    "for col in text_cols:\n",
    "    print(f\"\\nColumn: {col}\")\n",
    "    print(df[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "018efda2-aaca-4543-941b-2521fe4663ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking unique values in each text column to spot inconsistencies or odd entries.\n",
    "# 'Education' looks fine, but 'Marital_Status' has some strange values like 'YOLO' and 'Absurd'.\n",
    "# 'Dt_Customer' contains many unique dates, which makes sense since each customer joined on a different day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5dd0634-ae85-47b8-b6b8-d22af53fbd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean & Standardize Text Columns\n",
    "# Education\n",
    "df['Education'] = df['Education'].replace({\n",
    "    '2n Cycle': 'Master',\n",
    "    'Basic': 'Undergraduate'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fce1f2c8-b0f2-4c77-85f2-b1cda239325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marital Status\n",
    "df['Marital_Status'] = df['Marital_Status'].replace({\n",
    "    'Alone': 'Single',\n",
    "    'YOLO': 'Single',\n",
    "    'Absurd': 'Single'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e51ab2a3-fbef-47b5-ba4f-7aa2edc1e9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'Education': ['Graduation' 'PhD' 'Master' 'Undergraduate']\n",
      "Unique values in 'Marital_Status': ['Single' 'Together' 'Married' 'Divorced' 'Widow']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Unique values in 'Education':\", df['Education'].unique())\n",
    "print(\"Unique values in 'Marital_Status':\", df['Marital_Status'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5b63921a-0f71-4429-a20c-e614322b8cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#So here we replace \"Alone\", \"YOLO\" , \"Absurd\" , With \"Single\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "abb7e88e-8502-4171-a7a9-78b3b5cc4537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converted Dt_Customer to datetime format.\n"
     ]
    }
   ],
   "source": [
    "# Here We Convert Date Columns to Datetime Format\n",
    "df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], format='%d-%m-%Y')\n",
    "print(\"\\nConverted Dt_Customer to datetime format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16e28ef5-0261-4679-a60e-907910212af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2012-09-04\n",
      "1   2014-03-08\n",
      "2   2013-08-21\n",
      "3   2014-02-10\n",
      "4   2014-01-19\n",
      "Name: Dt_Customer, dtype: datetime64[ns]\n",
      "\n",
      "Data type after conversion: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "print(df['Dt_Customer'].head())\n",
    "print(\"\\nData type after conversion:\", df['Dt_Customer'].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7d4fbb0c-e12e-464c-9840-ef1123c60034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'Year_Birth', 'Education', 'Marital_Status', 'Income', 'Kidhome', 'Teenhome', 'Dt_Customer', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2', 'Complain', 'Z_CostContact', 'Z_Revenue', 'Response']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7e7730fe-2d3d-4661-a902-830289fcee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So while Checking the columns I noticed that, There is no proper spacing for Eg(Teenhome , kidhome, etc) \n",
    "#And also some kind of \"mnt\" initials are attached which dont look good \n",
    "# So i Remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3e941358-5fe7-466b-915b-024bcaeef30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\n",
    "    'ID': 'customer_id',\n",
    "    'Year_Birth': 'year_of_birth',\n",
    "    'Education': 'education_level',\n",
    "    'Marital_Status': 'marital_status',\n",
    "    'Income': 'income',\n",
    "    'Kidhome': 'kids_at_home',\n",
    "    'Teenhome': 'teens_at_home',\n",
    "    'Dt_Customer': 'customer_since',\n",
    "    'Recency': 'days_since_last_purchase',\n",
    "    'MntWines': 'spend_wines',\n",
    "    'MntFruits': 'spend_fruits',\n",
    "    'MntMeatProducts': 'spend_meat',\n",
    "    'MntFishProducts': 'spend_fish',\n",
    "    'MntSweetProducts': 'spend_sweets',\n",
    "    'MntGoldProds': 'spend_gold',\n",
    "    'NumDealsPurchases': 'purchases_with_deals',\n",
    "    'NumWebPurchases': 'purchases_online',\n",
    "    'NumCatalogPurchases': 'purchases_catalog',\n",
    "    'NumStorePurchases': 'purchases_store',\n",
    "    'NumWebVisitsMonth': 'web_visits_month',\n",
    "    'AcceptedCmp3': 'accepted_campaign_3',\n",
    "    'AcceptedCmp4': 'accepted_campaign_4',\n",
    "    'AcceptedCmp5': 'accepted_campaign_5',\n",
    "    'AcceptedCmp1': 'accepted_campaign_1',\n",
    "    'AcceptedCmp2': 'accepted_campaign_2',\n",
    "    'Complain': 'complained',\n",
    "    'Z_CostContact': 'cost_contact',\n",
    "    'Z_Revenue': 'revenue',\n",
    "    'Response': 'overall_response'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "471c875a-63e9-4fbd-bf07-0d1a604549f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['customer_id', 'year_of_birth', 'education_level', 'marital_status', 'income', 'kids_at_home', 'teens_at_home', 'customer_since', 'days_since_last_purchase', 'spend_wines', 'spend_fruits', 'spend_meat', 'spend_fish', 'spend_sweets', 'spend_gold', 'purchases_with_deals', 'purchases_online', 'purchases_catalog', 'purchases_store', 'web_visits_month', 'accepted_campaign_3', 'accepted_campaign_4', 'accepted_campaign_5', 'accepted_campaign_1', 'accepted_campaign_2', 'complained', 'cost_contact', 'revenue', 'overall_response']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "037d509e-5212-48bd-b61d-406bc1d466ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now they Look Better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5f35bd32-af93-42f3-9e3e-1d28a82bddcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Types after Cleaning:\n",
      "customer_id                          int64\n",
      "year_of_birth                        int64\n",
      "education_level                     object\n",
      "marital_status                      object\n",
      "income                             float64\n",
      "kids_at_home                         int64\n",
      "teens_at_home                        int64\n",
      "customer_since              datetime64[ns]\n",
      "days_since_last_purchase             int64\n",
      "spend_wines                          int64\n",
      "spend_fruits                         int64\n",
      "spend_meat                           int64\n",
      "spend_fish                           int64\n",
      "spend_sweets                         int64\n",
      "spend_gold                           int64\n",
      "purchases_with_deals                 int64\n",
      "purchases_online                     int64\n",
      "purchases_catalog                    int64\n",
      "purchases_store                      int64\n",
      "web_visits_month                     int64\n",
      "accepted_campaign_3                  int64\n",
      "accepted_campaign_4                  int64\n",
      "accepted_campaign_5                  int64\n",
      "accepted_campaign_1                  int64\n",
      "accepted_campaign_2                  int64\n",
      "complained                           int64\n",
      "cost_contact                         int64\n",
      "revenue                              int64\n",
      "overall_response                     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nData Types after Cleaning:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a6b21849-6f6d-4f0a-be81-4f2fa193933f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Cleaning Completed Successfully!\n",
      "Final Shape: 2240 rows × 29 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nData Cleaning Completed Successfully!\")\n",
    "print(f\"Final Shape: {df.shape[0]} rows × {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "70ebb41c-2171-4017-b298-b41dc57d1e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned dataset saved successfully as 'cleaned_marketing_campaign.csv'\n"
     ]
    }
   ],
   "source": [
    "output_path = \"cleaned_marketing_campaign.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"\\nCleaned dataset saved successfully as '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4f851455-8a41-4410-a5d5-1e50876afb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customer_id  year_of_birth education_level marital_status   income  \\\n",
      "0         5524           1957      Graduation         Single  58138.0   \n",
      "1         2174           1954      Graduation         Single  46344.0   \n",
      "2         4141           1965      Graduation       Together  71613.0   \n",
      "3         6182           1984      Graduation       Together  26646.0   \n",
      "4         5324           1981             PhD        Married  58293.0   \n",
      "\n",
      "   kids_at_home  teens_at_home customer_since  days_since_last_purchase  \\\n",
      "0             0              0     2012-09-04                        58   \n",
      "1             1              1     2014-03-08                        38   \n",
      "2             0              0     2013-08-21                        26   \n",
      "3             1              0     2014-02-10                        26   \n",
      "4             1              0     2014-01-19                        94   \n",
      "\n",
      "   spend_wines  ...  web_visits_month  accepted_campaign_3  \\\n",
      "0          635  ...                 7                    0   \n",
      "1           11  ...                 5                    0   \n",
      "2          426  ...                 4                    0   \n",
      "3           11  ...                 6                    0   \n",
      "4          173  ...                 5                    0   \n",
      "\n",
      "   accepted_campaign_4  accepted_campaign_5  accepted_campaign_1  \\\n",
      "0                    0                    0                    0   \n",
      "1                    0                    0                    0   \n",
      "2                    0                    0                    0   \n",
      "3                    0                    0                    0   \n",
      "4                    0                    0                    0   \n",
      "\n",
      "   accepted_campaign_2  complained  cost_contact  revenue  overall_response  \n",
      "0                    0           0             3       11                 1  \n",
      "1                    0           0             3       11                 0  \n",
      "2                    0           0             3       11                 0  \n",
      "3                    0           0             3       11                 0  \n",
      "4                    0           0             3       11                 0  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9470ce5-5046-41f0-9864-2f7424b66f98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
